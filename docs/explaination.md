## General framework
![](img/triangulation.png)  
### Measures:
- accuracy
- egises
- perseval

### Dataset structure:
Any dataset for which we will calculate above measures should have following:
- Documents(doc_id, title, text)
- personalized summaries generated by the user
- corresponding personalized summaries generated by model for that user

Sometimes we have all 3 (for eg [PENS dataset](https://github.com/LLluoling/PENS-Personalized-News-Headline-Generation),  
sometimes we have to appropriate it(eg: [openai/reddit dataset](https://huggingface.co/datasets/openai/summarize_from_feedback))

### Measures:
Calculation of personalization measures(egises, perseval) requires a base measure i.e the way the distance between nodes will be calculated.
We have so far worked with following measures:

| Measure                                                        | Description                                                                                                        |
|----------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------|
| [METEOR](https://huggingface.co/spaces/evaluate-metric/meteor) | Considers synonyms, stemming, and word order; combines precision and recall                                        |
| [BLEU](https://en.wikipedia.org/wiki/BLEU)                                                       | Based on n-gram precision; compares overlapping n-grams between reference and candidate texts                      |
| [ROUGE-L](https://en.wikipedia.org/wiki/ROUGE_(metric))                                                    | Based on longest common subsequence (LCS); considers sentence-level structure similarity                          |
| [ROUGE-SU4](https://en.wikipedia.org/wiki/ROUGE_(metric))                                                  | Extension of ROUGE-S; considers skip-bigrams with up to 4 words in between, including unigrams                     |
| [InfoLM](https://lightning.ai/docs/torchmetrics/stable/text/infolm.html)                                                     | Uses information-theoretic measures to evaluate language models' predictions                                       |
| [JSD](https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence)                                                        | Jensen-Shannon Divergence; measures similarity between two probability distributions                               |
| [BERTScore](https://huggingface.co/spaces/evaluate-metric/bertscore)                                                  | Uses BERT embeddings to evaluate text similarity; computes cosine similarity between contextual embeddings         |


Scores(egises, perseval) are characterized by _(model, measure)_ 


### End goal: Given a personalized model, measure, and a dataset calculate personalized summarization scores(egises,perseval)  for a set of documents

### Data Models
To represent the entites involved in calucation we use following models
1. [Document](https://github.com/KDM-LAB/egises/blob/881e1c6ecf60dc1a501a40311a867a1f9b81669a/egises/models.py#L41): represents a document. Contains it"s user summaries as well as model summaries.  
2. [Summary](https://github.com/KDM-LAB/egises/blob/881e1c6ecf60dc1a501a40311a867a1f9b81669a/egises/models.py#L30): represents a summary either generated by user or model 
3. [Egises](https://github.com/KDM-LAB/egises/blob/881e1c6ecf60dc1a501a40311a867a1f9b81669a/egises/models.py#L112): class responsible for populating distances as well as measures(accuracy, egises, perseval))

### Calculations
#### Distance calculation
**Input file:** consolidated jsonl file (refer sample [here]())  
Each line representing a document(doc_id, doc_id, doc_summ,m_summ_dict) where m_summ_dict contains all user generated summaries as well as model generated summaries
**Arguments:** model_name, measure, documents(derived from input file)

To calculate either of accuracy, egises or perseval, we need to calculate: 
- user summary distances: (u_i1, u_i2) summary generated for Document, D_i by user u_1 and u_2 (green edge)  
_charecterized by:  (doc_id, origin_model, u1, u2)_
- model summary distances (s_i1, s_i2) summary generated for Document, D_i by model s for u_1 and u_2 (blue dashed edge)  
_charecterized by:  (doc_id, origin_model, u1, u2)_
- user-document distances: distance between user generated summary and the Document 
_charecterized by (doc_id, origin_model, u1) where origin_model="user"_
- model-document distances: distance between model generated summary and document  
_charecterized by (doc_id, origin_model, u1) where origin_model="model_name"_

**Output:**  3 files
- sum_doc_distances.csv (doc_id,origin_model,uid,score)
- sum_sum_doc_distances.csv (doc_id,origin_model,uid1,uid2,score)
- sum_user_distances.csv (doc_id,origin_model,uid,score)
 
Once you have these files for a (measure, model), you can easily calculate the personalization scores(accuracy, egises, perseval)

#### Egises

#### Accuracy

#### Perseval



